# CONTEXTO Y ESTADO ACTUAL DEL PROYECTO (17/06/2025)

## Resumen de los últimos cambios y contexto de trabajo
- El proyecto se desarrolla y mantiene principalmente en VS Code, usando integración con GitHub, Google Colab y Google Drive para colaboración, entrenamiento y pruebas multiplataforma.
- El flujo de trabajo incluye edición y ejecución de scripts en local (VS Code), entrenamiento de modelos ML en Colab, y almacenamiento/backup de datos y modelos en Drive.
- Se han realizado múltiples pruebas y refactorizaciones para asegurar rutas multiplataforma y robustas.

## Scripts y archivos clave para Machine Learning y backtesting
- **train_model.py**: Orquesta el pipeline de procesamiento de datos, entrenamiento y guardado de modelos ML (Random Forest). Ahora soporta carga de múltiples archivos históricos fragmentados.
- **binance_data_processor.py**: Procesa los datos brutos, genera features e indicadores, y calcula la variable objetivo (`target`).
- **run_ml_backtest.py**: Ejecuta el backtesting usando el modelo ML entrenado y los datos procesados.
- **binance_data/**: Carpeta donde se almacenan los archivos históricos fragmentados (`SOLUSDT_raw_historical_data_*.csv`).
- **processed_data/**: Carpeta de salida para datasets procesados.
- **ml_models/**: Carpeta de salida para modelos y escaladores entrenados.
- **info/**: Carpeta de logs, documentación y registros de estado.

## Estado actual y problemas detectados
- El pipeline concatena y procesa correctamente los archivos históricos.
- El cálculo del target sigue generando una sola clase, lo que impide el entrenamiento ML (SMOTE y RandomForest requieren al menos dos clases).
- Se han implementado logs y archivos de depuración, pero el problema persiste incluso con umbral 0.0.
- Se han probado diferentes volúmenes de datos (todos los archivos, primeros 3, solo el primero) sin éxito.

## Próximos pasos sugeridos
1. Verificar manualmente la variación de la columna `close` en los datos brutos.
2. Revisar la lógica de cálculo de `future_return` y `target` en `binance_data_processor.py`.
3. Probar con otros símbolos o fuentes de datos para descartar problemas de calidad de datos.
4. Automatizar la generación de fragmentos de depuración para análisis manual.

## Herramientas y plataformas utilizadas
- **VS Code**: Edición, ejecución y depuración de scripts.
- **GitHub**: Control de versiones y colaboración.
- **Google Colab**: Entrenamiento de modelos ML con recursos de GPU.
- **Google Drive**: Backup y sincronización de datos/modelos.

## Contexto para otras IA o programadores
- El proyecto está en fase de integración y depuración del pipeline ML.
- El principal cuello de botella es la generación de la variable objetivo (`target`) y la calidad de los datos históricos.
- Toda la información relevante de cambios, rutas y estado se documenta en la carpeta `info/`.
- El objetivo inmediato es lograr un pipeline de ML funcional de principio a fin, con datos robustos y modelos entrenables.

---

_Archivo generado automáticamente por GitHub Copilot el 17/06/2025 para contexto de IA y futuros programadores._
